# Future Directions: AI-Powered File Organization

This document outlines future enhancements for TidyUp that leverage AI/LLM capabilities for smarter file categorization. These features are planned for post-v1.0 releases.

## Current State (v1.0)

TidyUp uses a layered detection approach:

```
File â†’ Detector Chain â†’ Category â†’ Folder
```

**Detectors** are rule-based and deterministic:
- ScreenshotDetector: Filename patterns
- InvoiceDetector: PDF content keywords
- BookDetector: ISBN, ebook extensions
- ArxivDetector: Paper ID patterns
- GenericDetector: Extension mapping (fallback)

**Limitation:** Detectors return hardcoded category names. Custom categories require manual configuration.

---

## Planned Enhancements

### Level 4: LLM-Powered Detection

Use a language model to classify files semantically when rule-based detection is uncertain.

**How it works:**

```
File: "quarterly_strategy_review.pdf"
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Rule-based detectors run first                             â”‚
â”‚  GenericDetector â†’ "Documents" (low confidence)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (confidence < threshold)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Classification                                          â”‚
â”‚                                                              â”‚
â”‚  System: Classify this file into one of the user's          â”‚
â”‚          categories: [Documents, Work, Client Projects,     â”‚
â”‚          Personal, Finance, ...]                            â”‚
â”‚                                                              â”‚
â”‚  File: quarterly_strategy_review.pdf                        â”‚
â”‚  Content preview: "Q3 2024 Strategic Review... revenue      â”‚
â”‚  targets... market expansion..."                            â”‚
â”‚                                                              â”‚
â”‚  Response: "Work" (confidence: 0.92)                        â”‚
â”‚  Reasoning: "Business strategy document, quarterly review"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Capabilities:**

| Use Case | Rule-Based | LLM-Powered |
|----------|-----------|-------------|
| Known patterns (screenshots) | âœ… Fast, accurate | Overkill |
| Technical vs Fiction books | âŒ Can't distinguish | âœ… Understands content |
| Novel category names | âŒ No rules exist | âœ… Infers from description |
| Ambiguous files | Falls to Unsorted | âœ… Makes educated guess |

**Configuration:**

```yaml
detection:
  llm:
    enabled: true
    provider: anthropic  # or openai, local
    model: claude-3-haiku  # Fast, cheap for classification
    threshold: 0.6  # Use LLM when rule confidence < 60%
    cache: true  # Cache results by file hash
    max_content_chars: 2000  # Limit content sent to API
    fallback_on_error: Unsorted
```

---

### Level 5: Hybrid Architecture

The full vision combines all approaches in a confidence-based pipeline:

```
File: "document.pdf"
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: Fast Deterministic (existing detectors)           â”‚
â”‚  Priority-ordered detector chain                            â”‚
â”‚  â†’ High confidence (>90%)? DONE                             â”‚
â”‚  â†’ Medium confidence? Continue to Layer 2                   â”‚
â”‚  â†’ No match? Continue to Layer 2                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: User Rules (config-based)                         â”‚
â”‚  Check category rules (keywords, patterns)                  â”‚
â”‚  â†’ Match found? DONE                                         â”‚
â”‚  â†’ No match? Continue to Layer 3                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: LLM Classification (if enabled)                   â”‚
â”‚  Send file info + content preview to LLM                    â”‚
â”‚  â†’ Returns category + confidence + reasoning                â”‚
â”‚  â†’ Cache result for identical files                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 4: Fallback                                          â”‚
â”‚  GenericDetector extension mapping â†’ "Unsorted"             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits of hybrid approach:**
- Fast for common cases (no API calls)
- LLM only used when needed
- Graceful degradation offline
- User rules take precedence over LLM

---

## Implementation Considerations

### API Costs and Latency

| Model | Cost per 1K tokens | Latency | Use Case |
|-------|-------------------|---------|----------|
| claude-3-haiku | ~$0.00025 | ~200ms | Best for classification |
| claude-3-sonnet | ~$0.003 | ~500ms | Complex reasoning |
| gpt-4o-mini | ~$0.00015 | ~300ms | Alternative |
| Local (Ollama) | Free | ~1-5s | Privacy-focused |

**Cost estimate:** Organizing 1000 files with 20% needing LLM = ~200 API calls = ~$0.05

### Privacy Considerations

```yaml
detection:
  llm:
    # What to send to API
    send_filename: true
    send_content: true
    max_content_chars: 2000  # Limit exposure

    # Privacy mode - only send metadata
    privacy_mode: false  # If true, only send filename and extension
```

**Privacy modes:**
1. **Full content:** Send filename + content preview (best accuracy)
2. **Metadata only:** Send filename + extension (reduced accuracy)
3. **Local only:** Use local LLM (Ollama) for complete privacy

### Caching Strategy

```python
# Cache by file hash to avoid re-classifying identical files
cache_key = f"{file_hash}:{categories_hash}"

# Cache stores:
# - Category result
# - Confidence score
# - LLM reasoning (for debugging)
# - Timestamp

# Cache invalidation:
# - File content changes (different hash)
# - Category list changes (different categories_hash)
# - Manual cache clear
```

### Offline Fallback

When LLM is unavailable:
1. Use cached results if available
2. Fall back to rule-based detection
3. Route to Unsorted with low confidence flag
4. Queue for LLM classification when online

---

## Alternative Approaches Considered

### WordNet/Thesaurus Lookup

```python
from nltk.corpus import wordnet

# "Technical Books" â†’ find related terms
# "technical" â†’ specialized, technological
# "book" â†’ textbook, manual, novel
```

**Rejected because:**
- ~100MB NLTK dependency
- Noisy results (too many synonyms)
- Doesn't understand context

### Word Embeddings

```python
from gensim.models import KeyedVectors

# Find semantically similar words
model.most_similar("technical", topn=10)
# â†’ engineering, scientific, professional
```

**Rejected because:**
- 1-3GB model file
- Overkill for this use case
- Doesn't understand category intent

### Corpus-Based Learning

```python
# Analyze user's existing organized files
# Learn patterns from their organization
# Suggest based on similar files
```

**Deferred because:**
- Complex implementation
- Requires existing organized files
- Cold start problem

---

## Roadmap

| Phase | Feature | Status |
|-------|---------|--------|
| Current | Rule-based detection | âœ… Complete |
| v1.1 | Level 1: Remap | ğŸ”² Planned |
| v1.1 | Level 2: Config rules | ğŸ”² Planned |
| v1.1 | Level 3a: Static dictionary suggestions | ğŸ”² Planned |
| v1.2 | Level 4: LLM integration (optional) | ğŸ”² Future |
| v1.3 | Level 5: Hybrid architecture | ğŸ”² Future |
| v2.0 | Local LLM support (Ollama) | ğŸ”² Future |

---

## Configuration Reference (Future)

```yaml
# ~/.tidy/config.yaml - Future LLM configuration

detection:
  llm:
    # Enable/disable LLM detection
    enabled: false

    # Provider configuration
    provider: anthropic  # anthropic, openai, ollama
    model: claude-3-haiku
    api_key_env: ANTHROPIC_API_KEY  # Read from environment

    # When to use LLM
    threshold: 0.6  # Use when rule confidence < 60%
    always_for: [Unsorted]  # Always try LLM for these categories

    # Performance
    cache: true
    cache_ttl_days: 30
    timeout_seconds: 10
    max_retries: 2

    # Privacy
    send_content: true
    max_content_chars: 2000
    privacy_mode: false  # If true, only send filename

    # Fallback
    fallback_on_error: Unsorted
    offline_mode: cache_only  # cache_only, rules_only, skip

# Category descriptions for LLM context
categories:
  - name: Technical Books
    description: "Books about programming, software development, technology"

  - name: Client Projects
    description: "Documents related to client work and consulting projects"
```

---

## Related Documentation

- [BACKLOG.md](BACKLOG.md) - Implementation tasks
- [USER_GUIDE.md](USER_GUIDE.md) - Current feature documentation
- [DEVELOPMENT.md](DEVELOPMENT.md) - Contributing guide
